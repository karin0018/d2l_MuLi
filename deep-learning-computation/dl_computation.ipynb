{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1729c6",
   "metadata": {},
   "source": [
    "## 多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c26fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1506, -0.1721,  0.0328,  0.0273,  0.0705,  0.0432, -0.2675,  0.0705,\n",
       "          0.0802, -0.0505],\n",
       "        [ 0.1168, -0.2236, -0.0185,  0.1986,  0.0265,  0.0079, -0.2239,  0.0575,\n",
       "          0.0282, -0.1293]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "\n",
    "# 2 是批量大小，20 是输入数据的维度\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63775eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(10,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "106b31b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1399,  0.1329,  0.0407, -0.3097, -0.1972, -0.1405, -0.1174,  0.1585,\n",
       "         -0.0074, -0.1313],\n",
       "        [ 0.0587,  0.3882, -0.0097, -0.2372, -0.1299, -0.1267, -0.0188,  0.1376,\n",
       "         -0.0616, -0.0951],\n",
       "        [ 0.2029,  0.1268,  0.0130, -0.2710, -0.2110, -0.1919, -0.1457,  0.0541,\n",
       "          0.0454, -0.0837]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(3,10)\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92647b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(Linear(in_features=10, out_features=256, bias=True), Linear(in_features=10, out_features=256, bias=True)), (ReLU(), ReLU()), (Linear(in_features=256, out_features=10, bias=True), Linear(in_features=256, out_features=10, bias=True))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2133,  0.0457, -0.1385,  0.0625, -0.1255, -0.0967,  0.1359,  0.1135,\n",
       "          0.0552, -0.1662],\n",
       "        [ 0.1382,  0.0887,  0.0256,  0.2528, -0.0623, -0.0800,  0.1204,  0.1802,\n",
       "          0.1139, -0.1001],\n",
       "        [ 0.2742,  0.1356, -0.1193,  0.0438, -0.1259, -0.1257,  0.1254,  0.0872,\n",
       "          0.0968, -0.1817]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"顺序执行所有类\"\"\"\n",
    "        print(self._modules)\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "net = MySequential(nn.Linear(10,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe5ae5",
   "metadata": {},
   "source": [
    "灵活的前向计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aae69e6c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1688, -0.0479,  0.3440, -0.3189,  0.1951,  0.1411, -0.0314,  0.2381,\n",
      "          0.1274,  0.3521,  0.2575, -0.5537, -0.4296,  0.1636, -0.0822, -0.0152,\n",
      "         -0.4712,  0.3673, -0.3553,  0.1399],\n",
      "        [-0.2046, -0.1297,  0.4285, -0.2998,  0.2628,  0.1910, -0.2557,  0.2244,\n",
      "          0.2220,  0.6431,  0.1826, -0.7076, -0.4830,  0.4011,  0.0918,  0.0085,\n",
      "         -0.7604,  0.4925, -0.5370,  0.2585]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.1191, grad_fn=<SumBackward0>)\n",
      "tensor([[-0.0844, -0.0239,  0.1720, -0.1595,  0.0976,  0.0705, -0.0157,  0.1191,\n",
      "          0.0637,  0.1760,  0.1287, -0.2768, -0.2148,  0.0818, -0.0411, -0.0076,\n",
      "         -0.2356,  0.1837, -0.1776,  0.0699],\n",
      "        [-0.1023, -0.0648,  0.2142, -0.1499,  0.1314,  0.0955, -0.1279,  0.1122,\n",
      "          0.1110,  0.3215,  0.0913, -0.3538, -0.2415,  0.2005,  0.0459,  0.0043,\n",
      "         -0.3802,  0.2463, -0.2685,  0.1292]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.0596, grad_fn=<SumBackward0>)\n",
      "tensor([[-0.0422, -0.0120,  0.0860, -0.0797,  0.0488,  0.0353, -0.0078,  0.0595,\n",
      "          0.0318,  0.0880,  0.0644, -0.1384, -0.1074,  0.0409, -0.0206, -0.0038,\n",
      "         -0.1178,  0.0918, -0.0888,  0.0350],\n",
      "        [-0.0512, -0.0324,  0.1071, -0.0750,  0.0657,  0.0478, -0.0639,  0.0561,\n",
      "          0.0555,  0.1608,  0.0456, -0.1769, -0.1207,  0.1003,  0.0229,  0.0021,\n",
      "         -0.1901,  0.1231, -0.1342,  0.0646]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.0298, grad_fn=<SumBackward0>)\n",
      "tensor([[-0.0211, -0.0060,  0.0430, -0.0399,  0.0244,  0.0176, -0.0039,  0.0298,\n",
      "          0.0159,  0.0440,  0.0322, -0.0692, -0.0537,  0.0205, -0.0103, -0.0019,\n",
      "         -0.0589,  0.0459, -0.0444,  0.0175],\n",
      "        [-0.0256, -0.0162,  0.0536, -0.0375,  0.0329,  0.0239, -0.0320,  0.0281,\n",
      "          0.0277,  0.0804,  0.0228, -0.0884, -0.0604,  0.0501,  0.0115,  0.0011,\n",
      "         -0.0951,  0.0616, -0.0671,  0.0323]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.0149, grad_fn=<SumBackward0>)\n",
      "tensor([[-0.0105, -0.0030,  0.0215, -0.0199,  0.0122,  0.0088, -0.0020,  0.0149,\n",
      "          0.0080,  0.0220,  0.0161, -0.0346, -0.0269,  0.0102, -0.0051, -0.0010,\n",
      "         -0.0294,  0.0230, -0.0222,  0.0087],\n",
      "        [-0.0128, -0.0081,  0.0268, -0.0187,  0.0164,  0.0119, -0.0160,  0.0140,\n",
      "          0.0139,  0.0402,  0.0114, -0.0442, -0.0302,  0.0251,  0.0057,  0.0005,\n",
      "         -0.0475,  0.0308, -0.0336,  0.0162]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.0074, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0074, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X = self.linear(X)\n",
    "       \n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2 \n",
    "            print(X)\n",
    "            print(X.sum())\n",
    "        return X.sum()\n",
    "\n",
    "X = torch.rand(2,20)            \n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b87355",
   "metadata": {},
   "source": [
    "混合搭配各种组合块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6324a19",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1415,  0.0149,  0.0335,  0.0573, -0.4084, -0.2105, -0.0499, -0.2289,\n",
      "         -0.0579,  0.2685,  0.0811,  0.0997, -0.3233,  0.2136,  0.0781,  0.2567,\n",
      "         -0.2134, -0.2654, -0.0771,  0.1084],\n",
      "        [ 0.1423,  0.0179,  0.0307,  0.0587, -0.4003, -0.2135, -0.0554, -0.2223,\n",
      "         -0.0542,  0.2639,  0.0856,  0.0947, -0.3176,  0.2073,  0.0708,  0.2503,\n",
      "         -0.2089, -0.2603, -0.0847,  0.1018]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.9745, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.0708,  0.0074,  0.0167,  0.0287, -0.2042, -0.1053, -0.0249, -0.1145,\n",
      "         -0.0289,  0.1343,  0.0405,  0.0498, -0.1616,  0.1068,  0.0391,  0.1284,\n",
      "         -0.1067, -0.1327, -0.0386,  0.0542],\n",
      "        [ 0.0711,  0.0089,  0.0154,  0.0293, -0.2001, -0.1067, -0.0277, -0.1111,\n",
      "         -0.0271,  0.1319,  0.0428,  0.0474, -0.1588,  0.1037,  0.0354,  0.1252,\n",
      "         -0.1044, -0.1302, -0.0423,  0.0509]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.4872, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.0354,  0.0037,  0.0084,  0.0143, -0.1021, -0.0526, -0.0125, -0.0572,\n",
      "         -0.0145,  0.0671,  0.0203,  0.0249, -0.0808,  0.0534,  0.0195,  0.0642,\n",
      "         -0.0534, -0.0663, -0.0193,  0.0271],\n",
      "        [ 0.0356,  0.0045,  0.0077,  0.0147, -0.1001, -0.0534, -0.0139, -0.0556,\n",
      "         -0.0136,  0.0660,  0.0214,  0.0237, -0.0794,  0.0518,  0.0177,  0.0626,\n",
      "         -0.0522, -0.0651, -0.0212,  0.0254]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.2436, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.0177,  0.0019,  0.0042,  0.0072, -0.0510, -0.0263, -0.0062, -0.0286,\n",
      "         -0.0072,  0.0336,  0.0101,  0.0125, -0.0404,  0.0267,  0.0098,  0.0321,\n",
      "         -0.0267, -0.0332, -0.0096,  0.0136],\n",
      "        [ 0.0178,  0.0022,  0.0038,  0.0073, -0.0500, -0.0267, -0.0069, -0.0278,\n",
      "         -0.0068,  0.0330,  0.0107,  0.0118, -0.0397,  0.0259,  0.0088,  0.0313,\n",
      "         -0.0261, -0.0325, -0.0106,  0.0127]], grad_fn=<DivBackward0>)\n",
      "tensor(-0.1218, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.1218, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db851be7",
   "metadata": {},
   "source": [
    "## 参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b42db7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0644],\n",
       "        [0.1803]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "\n",
    "# 2 是批量大小，20 是输入数据的维度\n",
    "X = torch.rand(2,4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77144b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0269,  0.2031,  0.2608, -0.3484,  0.2018, -0.1246,  0.3527, -0.3169]])), ('bias', tensor([0.2592]))])\n"
     ]
    }
   ],
   "source": [
    "# 查看标号为 2 的 net 的状态：weights ， bias\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4735be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2592], requires_grad=True)\n",
      "tensor([0.2592])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net[2].bias.grad)\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd210b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有参数\n",
    "print(*[(name,param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c429abe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2592])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09b3d264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4167],\n",
       "        [-0.4170]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从嵌套块收集参数\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.Tanh())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net \n",
    "\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68be9d69",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301cc950",
   "metadata": {},
   "source": [
    "### 内置参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25f1098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0070,  0.0074, -0.0188,  0.0182,  0.0082,  0.0157,  0.0082,  0.0011,\n",
       "          0.0055, -0.0058, -0.0197,  0.0009, -0.0170, -0.0015,  0.0133, -0.0019,\n",
       "          0.0040,  0.0055, -0.0075, -0.0030]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # 下划线表示替换函数，不返回值。 weight 不能全部初始化成常数\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "# 遍历整个神经网络，做 init_normal 操作\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6029dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0921, -0.0661, -0.1078,  0.0974, -0.1087,  0.0505, -0.0241, -0.1206,\n",
      "        -0.0129,  0.1390, -0.1018,  0.0274,  0.0128,  0.1253,  0.1404,  0.0556,\n",
      "        -0.0795,  0.0774, -0.0122,  0.0316])\n",
      "tensor([[42., 42., 42.,  ..., 42., 42., 42.],\n",
      "        [42., 42., 42.,  ..., 42., 42., 42.],\n",
      "        [42., 42., 42.,  ..., 42., 42., 42.],\n",
      "        ...,\n",
      "        [42., 42., 42.,  ..., 42., 42., 42.],\n",
      "        [42., 42., 42.,  ..., 42., 42., 42.],\n",
      "        [42., 42., 42.,  ..., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    # 优秀的初始化应该使得各层的激活值和状态梯度的方差在传播过程中的方差保持一致\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb406c7e",
   "metadata": {},
   "source": [
    "### 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c63af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([256, 20])\n",
      "Init weight torch.Size([10, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  8.7513,\n",
       "          5.5534, -0.0000, -0.0000, -8.8039, -8.4391, -0.0000,  7.5690,  0.0000,\n",
       "         -0.0000, -0.0000, -5.9012, -8.3093],\n",
       "        [-0.0000,  6.6263,  0.0000,  6.5707, -0.0000, -9.2991,  0.0000,  0.0000,\n",
       "         -0.0000, -9.8854,  6.7244, -5.1630,  0.0000,  8.2028,  8.9095,  0.0000,\n",
       "          6.7853, -0.0000,  0.0000,  6.2909]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efbcccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  9.7513,\n",
       "         6.5534,  1.0000,  1.0000, -7.8039, -7.4391,  1.0000,  8.5690,  1.0000,\n",
       "         1.0000,  1.0000, -4.9012, -7.3093])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1 # 可以直接做运算\n",
    "net[0].weight.data[0,0] = 42 # 也可以做替换\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a2306",
   "metadata": {},
   "source": [
    "### 参数绑定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03ab1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3,4)\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(), shared, \n",
    "                    nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "# shared 层的所有参数都是相同的\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "# 一层修改，其他位置的 shared 层共享修改\n",
    "net[2].weight.data[0, 0] = 100 \n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7374a8",
   "metadata": {},
   "source": [
    "## 自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832fcfe",
   "metadata": {},
   "source": [
    "构造一个没有任何参数的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb2ff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X - X.mean() \n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc8939",
   "metadata": {},
   "source": [
    "可以将自定义的层作为组件合并到构建更复杂的模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e62c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4130,  0.5728, -0.3892, -0.0608,  0.5987, -0.4191,  0.4672, -0.4810,\n",
       "          1.0546, -0.7164],\n",
       "        [-0.1821,  0.2579, -0.4987,  0.2238,  0.1947, -0.3534,  0.1362, -0.2239,\n",
       "          0.6122, -0.3855],\n",
       "        [-0.6761,  0.6874, -0.1975, -0.0564,  0.3003, -0.3501,  0.6639, -0.6144,\n",
       "          0.8944, -0.5816],\n",
       "        [-0.6511,  0.3101, -0.3287,  0.0239,  0.4241, -0.8036,  0.7564, -0.4005,\n",
       "          1.2376, -0.6335]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 10), CenteredLayer())\n",
    "\n",
    "Y = net(torch.rand(4,8))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14151e2e",
   "metadata": {},
   "source": [
    "## 读取文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180531ea",
   "metadata": {},
   "source": [
    "加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf001f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80eec79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "# save tensor\n",
    "torch.save(x, 'x-file')\n",
    "# load tensor\n",
    "x2 = torch.load(\"x-file\")\n",
    "x2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ddb4",
   "metadata": {},
   "source": [
    "存储一个张量列表，然后把他们读回内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc36ab58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb282a8",
   "metadata": {},
   "source": [
    "写入或读取从字符串映射到张量的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a84e664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x, 'y':y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491442c",
   "metadata": {},
   "source": [
    "加载和保存模型参数：**只需要存模型每层的参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4d3bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(10,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03f76c",
   "metadata": {},
   "source": [
    "将模型的参数存储为一个叫做 \"mlp.params\" 的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd4daa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "X = torch.rand(2,10)\n",
    "# net.forward(X)\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45fbd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dda5e7",
   "metadata": {},
   "source": [
    "实例化了原始多层感知机模型的一个备份。直接读取文件中存储的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d7a78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=10, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "# 保证 BN 层能够用全部训练数据的均值和方差，即测试过程中要保证 BN 层的均值和方差不变。\n",
    "clone.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36913cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
